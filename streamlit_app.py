import polars as pl
import duckdb
import streamlit as st
import os
import time
import logging
import io
import zipfile
from pathlib import Path
from typing import Dict, List, Tuple
import warnings

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
EXCEL_ROW_LIMIT = 1_000_000

class HighVolumeAutoPartsCatalog:
    
    def __init__(self):
        self.data_dir = Path("./auto_parts_data")
        self.data_dir.mkdir(exist_ok=True)
        self.db_path = self.data_dir / "catalog.duckdb"
        self.conn = duckdb.connect(database=str(self.db_path))
        self.setup_database()
        
    def setup_database(self):
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS oe_data (
                oe_number_norm VARCHAR PRIMARY KEY,
                oe_number VARCHAR,
                name VARCHAR,
                applicability VARCHAR,
                category VARCHAR
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS parts_data (
                artikul_norm VARCHAR,
                brand_norm VARCHAR,
                artikul VARCHAR,
                brand VARCHAR,
                multiplicity INTEGER,
                barcode VARCHAR,
                length DOUBLE, 
                width DOUBLE,
                height DOUBLE, 
                weight DOUBLE,
                image_url VARCHAR,
                dimensions_str VARCHAR,
                description VARCHAR,
                price DECIMAL(10, 2),
                PRIMARY KEY (artikul_norm, brand_norm)
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS cross_references (
                oe_number_norm VARCHAR,
                artikul_norm VARCHAR,
                brand_norm VARCHAR,
                PRIMARY KEY (oe_number_norm, artikul_norm, brand_norm)
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS prices (
                artikul_norm VARCHAR,
                brand_norm VARCHAR,
                quantity INTEGER,
                recommended_price DECIMAL(10, 2),
                PRIMARY KEY (artikul_norm, brand_norm)
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS markup_settings (
                id SERIAL PRIMARY KEY,
                brand VARCHAR UNIQUE,
                markup_percentage DECIMAL(5, 2)
            )
        """)
        
    def create_indexes(self):
        st.info("–°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞...")
        indexes = [
            "CREATE INDEX IF NOT EXISTS idx_oe_data_oe ON oe_data(oe_number_norm)",
            "CREATE INDEX IF NOT EXISTS idx_parts_data_keys ON parts_data(artikul_norm, brand_norm)",
            "CREATE INDEX IF NOT EXISTS idx_cross_oe ON cross_references(oe_number_norm)",
            "CREATE INDEX IF NOT EXISTS idx_cross_artikul ON cross_references(artikul_norm, brand_norm)"
        ]
        for index_sql in indexes:
            self.conn.execute(index_sql)
        st.success("–ò–Ω–¥–µ–∫—Å—ã —Å–æ–∑–¥–∞–Ω—ã.")

    @staticmethod
    def normalize_key(key_series: pl.Series) -> pl.Series:
        return (
            key_series
            .fill_null("")
            .cast(pl.Utf8)
            .str.replace_all("'", "")
            .str.replace_all(r"[^0-9A-Za-z–ê-–Ø–∞-—è–Å—ë`\-\s]", "")
            .str.replace_all(r"\s+", " ")
            .str.strip_chars()
            .str.to_lowercase()
        )

    @staticmethod
    def clean_values(value_series: pl.Series) -> pl.Series:
        return (
            value_series
            .fill_null("")
            .cast(pl.Utf8)
            .str.replace_all("'", "")
            .str.replace_all(r"[^0-9A-Za-z–ê-–Ø–∞-—è–Å—ë`\-\s]", "")
            .str.replace_all(r"\s+", " ")
            .str.strip_chars()
        )

    @staticmethod
    def determine_category_vectorized(name_series: pl.Series) -> pl.Series:
        categories_map = {
            '–§–∏–ª—å—Ç—Ä': '—Ñ–∏–ª—å—Ç—Ä|filter', 
            '–¢–æ—Ä–º–æ–∑–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞': '—Ç–æ—Ä–º–æ–∑|brake|–∫–æ–ª–æ–¥–∫|–¥–∏—Å–∫|—Å—É–ø–ø–æ—Ä—Ç',
            '–ü–æ–¥–≤–µ—Å–∫–∞': '–∞–º–æ—Ä—Ç–∏–∑–∞—Ç–æ—Ä|—Å—Ç–æ–π–∫|spring|–ø–æ–¥–≤–µ—Å–∫|–†—ã—á–∞–≥|–†—ã—á–∞–≥–∏|–®–∞—Ä–æ–≤–∞—è –æ–ø–æ—Ä–∞|–û–ø–æ—Ä–∞ —à–∞—Ä–æ–≤–∞—è|–°–∞–π–ª–µ–Ω—Ç–±–ª–æ–∫|–°—Ç—É–ø–∏—Ü|–ø–æ–¥—à–∏–ø–Ω–∏–∫ —Å—Ç—É–ø–∏—Ü—ã|–ø–æ–¥—à–∏–ø–Ω–∏–∫–∏ —Å—Ç—É–ø–∏—Ü—ã', 
            '–î–≤–∏–≥–∞—Ç–µ–ª—å': '–¥–≤–∏–≥–∞—Ç–µ–ª—å|engine|—Å–≤–µ—á|–ø–æ—Ä—à–µ–Ω—å|–∫–ª–∞–ø–∞–Ω',
            '–¢—Ä–∞–Ω—Å–º–∏—Å—Å–∏—è': '—Ç—Ä–∞–Ω—Å–º–∏—Å—Å–∏—è|—Å—Ü–µ–ø–ª–µ–Ω|–∫–æ—Ä–æ–±–∫|transmission', 
            '–≠–ª–µ–∫—Ç—Ä–∏–∫–∞': '–∞–∫–∫—É–º—É–ª—è—Ç–æ—Ä|–≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä|—Å—Ç–∞—Ä—Ç–µ—Ä|–ø—Ä–æ–≤–æ–¥|–ª–∞–º–ø',
            '–†—É–ª–µ–≤–æ–µ': '—Ä—É–ª–µ–≤–æ–π|—Ç—è–≥–∞|–Ω–∞–∫–æ–Ω–µ—á–Ω–∏–∫|steering', 
            '–í—ã—Ö–ª–æ–ø–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞': '–≥–ª—É—à–∏—Ç–µ–ª—å|–≥–ª—É—à–∏—Ç–µ–ª|–∫–∞—Ç–∞–ª–∏–∑–∞—Ç–æ—Ä|–≤—ã—Ö–ª–æ–ø|exhaust',
            '–û—Ö–ª–∞–∂–¥–µ–Ω–∏–µ': '—Ä–∞–¥–∏–∞—Ç–æ—Ä|–≤–µ–Ω—Ç–∏–ª—è—Ç–æ—Ä|—Ç–µ—Ä–º–æ—Å—Ç–∞—Ç|cooling', 
            '–¢–æ–ø–ª–∏–≤–æ': '—Ç–æ–ø–ª–∏–≤–Ω—ã–π|–±–µ–Ω–∑–æ–Ω–∞—Å–æ—Å|—Ñ–æ—Ä—Å—É–Ω–∫|fuel',
        }
        name_lower = name_series.str.to_lowercase()
        expr = pl.when(pl.lit(False)).then(pl.lit(None))
        for category, pattern in categories_map.items():
            expr = expr.when(name_lower.str.contains(pattern)).then(pl.lit(category))
        return expr.otherwise(pl.lit('–†–∞–∑–Ω–æ–µ')).alias('category')

    def detect_columns(self, actual_columns: List[str], expected_columns: List[str]) -> Dict[str, str]:
        mapping = {}
        column_variants = {
            'oe_number': ['oe –Ω–æ–º–µ—Ä', 'oe', '–æe', '–Ω–æ–º–µ—Ä', 'code', 'OE'], 
            'artikul': ['–∞—Ä—Ç–∏–∫—É–ª', 'article', 'sku'],
            'brand': ['–±—Ä–µ–Ω–¥', 'brand', '–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å', 'manufacturer'], 
            'name': ['–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ', '–Ω–∞–∑–≤–∞–Ω–∏–µ', 'name', '–æ–ø–∏—Å–∞–Ω–∏–µ', 'description'],
            'applicability': ['–ø—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å', '–∞–≤—Ç–æ–º–æ–±–∏–ª—å', 'vehicle', 'applicability'], 
            'barcode': ['—à—Ç—Ä–∏—Ö-–∫–æ–¥', 'barcode', '—à—Ç—Ä–∏—Ö–∫–æ–¥', 'ean', 'eac13'],
            'multiplicity': ['–∫—Ä–∞—Ç–Ω–æ—Å—Ç—å —à—Ç', '–∫—Ä–∞—Ç–Ω–æ—Å—Ç—å', 'multiplicity'], 
            'length': ['–¥–ª–∏–Ω–∞ (—Å–º)', '–¥–ª–∏–Ω–∞', 'length', '–¥–ª–∏–Ω–Ω–∞'],
            'width': ['—à–∏—Ä–∏–Ω–∞ (—Å–º)', '—à–∏—Ä–∏–Ω–∞', 'width'], 
            'height': ['–≤—ã—Å–æ—Ç–∞ (—Å–º)', '–≤—ã—Å–æ—Ç–∞', 'height'],
            'weight': ['–≤–µ—Å (–∫–≥)', '–≤–µ—Å, –∫–≥', '–≤–µ—Å', 'weight'], 
            'image_url': ['—Å—Å—ã–ª–∫–∞', 'url', '–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ', 'image', '–∫–∞—Ä—Ç–∏–Ω–∫–∞'],
            'dimensions_str': ['–≤–µ—Å–æ–≥–∞–±–∞—Ä–∏—Ç—ã', '—Ä–∞–∑–º–µ—Ä—ã', 'dimensions', 'size'], 
            'price': ['—Ü–µ–Ω–∞', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', 'price'],
            'quantity': ['–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ', 'amount', 'quantity']
        }
        actual_lower = {col.lower(): col for col in actual_columns}
        for expected in expected_columns:
            variants = [v.lower() for v in column_variants.get(expected, [expected])]
            for variant in variants:
                for actual_l, actual_orig in actual_lower.items():
                    if variant in actual_l:
                        mapping[actual_orig] = expected
                        break
                if expected in mapping.values():
                    break
        return mapping

    def read_and_prepare_file(self, file_path: str, file_type: str) -> pl.DataFrame:
        logger.info(f"–ù–∞—á–∏–Ω–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É —Ñ–∞–π–ª–∞: {file_type} ({file_path})")
        try:
            df = pl.read_excel(file_path, engine='calamine')
        except Exception as e:
            logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª {file_path}: {e}")
            return pl.DataFrame()

        schemas = {
            'oe': ['oe_number', 'artikul', 'brand', 'name', 'applicability'],
            'barcode': ['brand', 'artikul', 'barcode', 'multiplicity'],
            'dimensions': ['artikul', 'brand', 'length', 'width', 'height', 'weight', 'dimensions_str'],
            'images': ['artikul', 'brand', 'image_url'],
            'cross': ['oe_number', 'artikul', 'brand'],
            'prices': ['artikul', 'brand', 'quantity', 'recommended_price']
        }
        expected_cols = schemas.get(file_type, [])
        column_mapping = self.detect_columns(df.columns, expected_cols)
        df = df.rename(column_mapping)
        
        # –û—á–∏—Å—Ç–∏—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –æ—Ç –∞–ø–æ—Å—Ç—Ä–æ—Ñ–æ–≤ –∏ –º—É—Å–æ—Ä–∞ –Ω–∞ –≤—Ö–æ–¥–µ
        if 'artikul' in df.columns:
            df = df.with_columns(artikul=self.clean_values(pl.col('artikul')))
        if 'brand' in df.columns:
            df = df.with_columns(brand=self.clean_values(pl.col('brand')))
        if 'oe_number' in df.columns:
            df = df.with_columns(oe_number=self.clean_values(pl.col('oe_number')))
        
        key_cols = [col for col in ['oe_number', 'artikul', 'brand'] if col in df.columns]
        if key_cols:
            df = df.unique(subset=key_cols, keep='first')

        # –°–æ–∑–¥–∞—Ç—å –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –≤–µ—Ä—Å–∏–∏ –¥–ª—è –∫–ª—é—á–µ–π (–Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä)
        if 'artikul' in df.columns:
            df = df.with_columns(artikul_norm=self.normalize_key(pl.col('artikul')))
        if 'brand' in df.columns:
            df = df.with_columns(brand_norm=self.normalize_key(pl.col('brand')))
        if 'oe_number' in df.columns:
            df = df.with_columns(oe_number_norm=self.normalize_key(pl.col('oe_number')))
            
        return df

    def upsert_data(self, table_name: str, df: pl.DataFrame, pk: List[str]):
        if df.is_empty():
            return
        
        df = df.unique(keep='first')
        
        cols = df.columns
        pk_str = ", ".join(f'"{c}"' for c in pk)
        
        temp_view_name = f"temp_{table_name}_{int(time.time())}"
        self.conn.register(temp_view_name, df.to_arrow())
        
        update_cols = [col for col in cols if col not in pk]
        
        if not update_cols:
            on_conflict_action = "DO NOTHING"
        else:
            update_clause = ", ".join([f'"{col}" = excluded."{col}"' for col in update_cols])
            on_conflict_action = f"DO UPDATE SET {update_clause}"

        sql = f"""
        INSERT INTO {table_name}
        SELECT * FROM {temp_view_name}
        ON CONFLICT ({pk_str}) {on_conflict_action};
        """
        
        try:
            self.conn.execute(sql)
            logger.info(f"–£—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω–æ/–≤—Å—Ç–∞–≤–ª–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π –≤ —Ç–∞–±–ª–∏—Ü—É {table_name}.")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ UPSERT –≤ {table_name}: {e}")
            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø–∏—Å–∏ –≤ —Ç–∞–±–ª–∏—Ü—É {table_name}. –î–µ—Ç–∞–ª–∏ –≤ –ª–æ–≥–µ.")
        finally:
            self.conn.unregister(temp_view_name)

    def process_and_load_data(self, dataframes: Dict[str, pl.DataFrame]):
        st.info("üîÑ –ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ –±–∞–∑–µ...")
        
        steps = [s for s in ['oe', 'cross', 'parts', 'prices'] if s in dataframes or s == 'parts']
        num_steps = len(steps)
        progress_bar = st.progress(0, text="–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—é –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...")
        step_counter = 0

        if 'oe' in dataframes:
            step_counter += 1
            progress_bar.progress(step_counter / (num_steps + 1), text=f"({step_counter}/{num_steps}) –û–±—Ä–∞–±–æ—Ç–∫–∞ OE –¥–∞–Ω–Ω—ã—Ö...")
            df = dataframes['oe'].filter(pl.col('oe_number_norm') != "")
            
            oe_df = df.select(['oe_number_norm', 'oe_number', 'name', 'applicability']).unique(subset=['oe_number_norm'], keep='first')
            if 'name' in oe_df.columns:
                oe_df = oe_df.with_columns(self.determine_category_vectorized(pl.col('name')))
            else:
                oe_df = oe_df.with_columns(category=pl.lit('–†–∞–∑–Ω–æ–µ'))
            self.upsert_data('oe_data', oe_df, ['oe_number_norm'])
            
            cross_df_from_oe = df.filter(pl.col('artikul_norm') != "").select(['oe_number_norm', 'artikul_norm', 'brand_norm']).unique()
            self.upsert_data('cross_references', cross_df_from_oe, ['oe_number_norm', 'artikul_norm', 'brand_norm'])

        if 'cross' in dataframes:
            step_counter += 1
            progress_bar.progress(step_counter / (num_steps + 1), text=f"({step_counter}/{num_steps}) –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫—Ä–æ—Å—Å–æ–≤...")
            df = dataframes['cross'].filter((pl.col('oe_number_norm') != "") & (pl.col('artikul_norm') != ""))
            cross_df_from_cross = df.select(['oe_number_norm', 'artikul_norm', 'brand_norm']).unique()
            self.upsert_data('cross_references', cross_df_from_cross, ['oe_number_norm', 'artikul_norm', 'brand_norm'])

        step_counter += 1
        progress_bar.progress(step_counter / (num_steps + 1), text=f"({step_counter}/{num_steps}) –°–±–æ—Ä–∫–∞ –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø–æ –∞—Ä—Ç–∏–∫—É–ª–∞–º...")
        parts_df = None
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ—Ä—è–¥–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ –¥–∞–Ω–Ω—ã—Ö
        # –ü–æ—Ä—è–¥–æ–∫ –≤–∞–∂–µ–Ω: —Å–Ω–∞—á–∞–ª–∞ –±–∞–∑–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ, –ø–æ—Ç–æ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ (dimensions –∏–º–µ–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        file_priority = ['oe', 'barcode', 'images', 'dimensions']
        key_files = {ftype: df for ftype, df in dataframes.items() if ftype in file_priority}
        
        if key_files:
            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∞—Ä—Ç–∏–∫—É–ª—ã –∏–∑ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤
            all_parts = pl.concat([
                df.select(['artikul', 'artikul_norm', 'brand', 'brand_norm']) 
                for df in key_files.values() if 'artikul_norm' in df.columns and 'brand_norm' in df.columns
            ]).filter(pl.col('artikul_norm') != "").unique(subset=['artikul_norm', 'brand_norm'], keep='first')

            parts_df = all_parts

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª—ã –≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ –¥–∞–Ω–Ω—ã—Ö
            for ftype in file_priority:
                if ftype not in key_files: continue
                df = key_files[ftype]
                if df.is_empty() or 'artikul_norm' not in df.columns: continue
                
                join_cols = [col for col in df.columns if col not in ['artikul', 'artikul_norm', 'brand', 'brand_norm']]
                if not join_cols: continue
                
                # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–æ–ª–æ–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ —É–∂–µ –µ—Å—Ç—å –≤ parts_df, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è
                existing_cols = set(parts_df.columns)
                join_cols = [col for col in join_cols if col not in existing_cols]
                if not join_cols: continue
                
                df_subset = df.select(['artikul_norm', 'brand_norm'] + join_cols).unique(subset=['artikul_norm', 'brand_norm'], keep='first')
                # coalesce=True –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∫–æ–ª–æ–Ω–æ–∫
                # –°—É—Ñ—Ñ–∏–∫—Å—ã –Ω–µ —Å–æ–∑–¥–∞—é—Ç—Å—è, —Ç–∞–∫ –∫–∞–∫ –º—ã —É–∂–µ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–ª–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏
                parts_df = parts_df.join(df_subset, on=['artikul_norm', 'brand_norm'], how='left', coalesce=True)

        if parts_df is not None and not parts_df.is_empty():
            # –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ multiplicity
            if 'multiplicity' not in parts_df.columns:
                parts_df = parts_df.with_columns(multiplicity=pl.lit(1).cast(pl.Int32))
            else:
                parts_df = parts_df.with_columns(
                    pl.col('multiplicity').fill_null(1).cast(pl.Int32)
                )
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–∞–∑–º–µ—Ä–æ–≤ –∏ –æ–ø–∏—Å–∞–Ω–∏–µ
            for col in ['length', 'width', 'height']:
                if col not in parts_df.columns:
                    parts_df = parts_df.with_columns(pl.lit(None).cast(pl.Float64).alias(col))
            # –°–æ–∑–¥–∞—Ç—å —Å—Ç—Ä–æ–∫–∏ —Ä–∞–∑–º–µ—Ä–æ–≤
            parts_df = parts_df.with_columns([
                pl.col('length').cast(pl.Utf8).fill_null('').alias('_length_str'),
                pl.col('width').cast(pl.Utf8).fill_null('').alias('_width_str'),
                pl.col('height').cast(pl.Utf8).fill_null('').alias('_height_str'),
            ])
            # –°–æ–∑–¥–∞–≤–∞—Ç—å –æ–ø–∏—Å–∞–Ω–∏–µ
            parts_df = parts_df.with_columns(
                description=pl.concat_str(
                    [
                        pl.lit('–ê—Ä—Ç–∏–∫—É–ª: '), pl.col('_length_str'),
                        pl.lit(', –ë—Ä–µ–Ω–¥: '), pl.col('_width_str'),
                        pl.lit(', –ö—Ä–∞—Ç–Ω–æ—Å—Ç—å: '), pl.col('_height_str'), 
                        pl.lit(' —à—Ç.')
                    ],
                    separator=''
                )
            )
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
            parts_df = parts_df.drop(['_length_str', '_width_str', '_height_str'])
            final_columns = [
                'artikul_norm', 'brand_norm', 'artikul', 'brand', 'multiplicity', 'barcode', 
                'length', 'width', 'height', 'weight', 'image_url', 'dimensions_str', 'description'
            ]
            select_exprs = [pl.col(c) if c in parts_df.columns else pl.lit(None).alias(c) for c in final_columns]
            parts_df = parts_df.select(select_exprs)
            self.upsert_data('parts_data', parts_df, ['artikul_norm', 'brand_norm'])
        
        if 'prices' in dataframes:
            step_counter += 1
            progress_bar.progress(step_counter / (num_steps + 1), text=f"({step_counter}/{num_steps}) –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ü–µ–Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
            df = dataframes['prices'].filter((pl.col('artikul_norm') != "") & (pl.col('brand_norm') != ""))
            self.upsert_data('prices', df, ['artikul_norm', 'brand_norm'])
        
        progress_bar.progress(1.0, text="–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        time.sleep(1)
        progress_bar.empty()
        st.success("üíæ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ –±–∞–∑—É –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")

    def merge_all_data_parallel(self, file_paths: Dict[str, str]) -> Dict[str, any]:
        start_time = time.time()
        stats = {}
        st.info("üöÄ –ù–∞—á–∞–ª–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ —á—Ç–µ–Ω–∏—è –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ —Ñ–∞–π–ª–æ–≤...")
        n_files = len(file_paths)
        file_progress_bar = st.progress(0, text="–û–∂–∏–¥–∞–Ω–∏–µ...")
        dataframes = {}
        processed_files = 0
        with ThreadPoolExecutor() as executor:
            future_to_file = {executor.submit(self.read_and_prepare_file, path, ftype): ftype for ftype, path in file_paths.items()}
            for future in as_completed(future_to_file):
                ftype = future_to_file[future]
                try:
                    df = future.result()
                    if not df.is_empty():
                        dataframes[ftype] = df
                        st.success(f"‚úÖ –§–∞–π–ª '{ftype}' –ø—Ä–æ—á–∏—Ç–∞–Ω: {len(df):,} —Å—Ç—Ä–æ–∫.")
                    else:
                        st.warning(f"‚ö†Ô∏è –§–∞–π–ª '{ftype}' –ø—É—Å—Ç –∏–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å.")
                except Exception as e:
                    logger.exception(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ {ftype}")
                    st.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ {ftype}: {e}")
                finally:
                    processed_files += 1
                    file_progress_bar.progress(processed_files / n_files, text=f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞: {ftype} ({processed_files}/{n_files})")
        file_progress_bar.empty()

        if not dataframes:
            st.error("‚ùå –ù–∏ –æ–¥–∏–Ω —Ñ–∞–π–ª –Ω–µ –±—ã–ª –∑–∞–≥—Ä—É–∂–µ–Ω. –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞.")
            return {}

        self.process_and_load_data(dataframes)
        processing_time = time.time() - start_time
        total_records = self.get_total_records()
        stats['processing_time'] = processing_time
        stats['total_records'] = total_records
        st.success(f"üéâ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {processing_time:.2f} —Å–µ–∫—É–Ω–¥")
        st.success(f"üìä –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∞—Ä—Ç–∏–∫—É–ª–æ–≤ –≤ –±–∞–∑–µ: {total_records:,}")
        self.create_indexes()
        return stats
    
    def get_total_records(self) -> int:
        try:
            result = self.conn.execute("SELECT COUNT(*) FROM parts_data").fetchone()
            return result[0] if result else 0
        except (duckdb.Error, TypeError):
            return 0

    def get_export_query(self, exclusions=None, columns=None) -> str:
        exclusion_conditions = ''
        if exclusions:
            exclusion_conditions = f"AND NOT REGEXP_MATCHES(CONCAT(representative_name, '|', representative_applicability), '{exclusions}')"
        
        column_selection = '*'
        if columns:
            column_selection = ', '.join(columns)
        
        return rf"""
        WITH PartDetails AS (
            SELECT
                cr.artikul_norm,
                cr.brand_norm,
                STRING_AGG(DISTINCT regexp_replace(regexp_replace(o.oe_number, '''', ''), '[^0-9A-Za-z–ê-–Ø–∞-—è–Å—ë`\-\s]', '', 'g'), ', ') AS oe_list,
                ANY_VALUE(o.name) AS representative_name,
                ANY_VALUE(o.applicability) AS representative_applicability,
                ANY_VALUE(o.category) AS representative_category
            FROM cross_references cr
            JOIN oe_data o ON cr.oe_number_norm = o.oe_number_norm
            GROUP BY cr.artikul_norm, cr.brand_norm
        ),
        AllAnalogs AS (
            SELECT
                cr1.artikul_norm,
                cr1.brand_norm,
                STRING_AGG(DISTINCT regexp_replace(regexp_replace(p2.artikul, '''', ''), '[^0-9A-Za-z–ê-–Ø–∞-—è–Å—ë`\-\s]', '', 'g'), ', ') as analog_list
            FROM cross_references cr1
            JOIN cross_references cr2 ON cr1.oe_number_norm = cr2.oe_number_norm
            JOIN parts_data p2 ON cr2.artikul_norm = p2.artikul_norm AND cr2.brand_norm = p2.brand_norm
            WHERE cr1.artikul_norm != p2.artikul_norm OR cr1.brand_norm != p2.brand_norm
            GROUP BY cr1.artikul_norm, cr1.brand_norm
        )
        SELECT {column_selection}
        FROM parts_data p
        LEFT JOIN PartDetails pd ON p.artikul_norm = pd.artikul_norm AND p.brand_norm = pd.brand_norm
        LEFT JOIN AllAnalogs aa ON p.artikul_norm = aa.artikul_norm AND p.brand_norm = aa.brand_norm
        WHERE pd.oe_list IS NOT NULL
        {exclusion_conditions}
        ORDER BY p.brand, p.artikul
        """

    def export_to_csv_optimized(self, output_path: str, exclusions=None, columns=None) -> bool:
        total_records = self.conn.execute("SELECT count(*) FROM (SELECT DISTINCT artikul_norm, brand_norm FROM parts_data) AS t").fetchone()[0]
        if total_records == 0:
            st.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞")
            return False
        
        st.info(f"üì§ –≠–∫—Å–ø–æ—Ä—Ç {total_records:,} –∑–∞–ø–∏—Å–µ–π –≤ CSV...")
        try:
            query = self.get_export_query(exclusions, columns)
            df = self.conn.execute(query).pl()

            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —á–∏—Å–ª–æ–≤—ã–µ —Å—Ç–æ–ª–±—Ü—ã –≤ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
            dimension_cols = ["–î–ª–∏–Ω–Ω–∞", "–®–∏—Ä–∏–Ω–∞", "–í—ã—Å–æ—Ç–∞", "–í–µ—Å", "–î–ª–∏–Ω–Ω–∞/–®–∏—Ä–∏–Ω–∞/–í—ã—Å–æ—Ç–∞", "–ö—Ä–∞—Ç–Ω–æ—Å—Ç—å"]
            for col_name in dimension_cols:
                if col_name in df.columns:
                    df = df.with_columns(
                        pl.when(pl.col(col_name).is_not_null())
                        .then(pl.col(col_name).cast(pl.Utf8))
                        .otherwise(pl.lit(""))
                        .alias(col_name)
                    )

            buf = io.StringIO()
            df.write_csv(buf, separator=';')
            csv_text = buf.getvalue()

            with open(output_path, 'wb') as f:
                f.write(b'\xef\xbb\xbf')
                f.write(csv_text.encode('utf-8'))

            file_size = os.path.getsize(output_path) / (1024 * 1024)
            st.success(f"‚úÖ –î–∞–Ω–Ω—ã–µ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤ CSV: {output_path} ({file_size:.1f} –ú–ë)")
            return True
        except Exception as e:
            logger.exception("–û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ CSV")
            st.error(f"‚ùå –û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ CSV: {e}")
            return False
    
    def export_to_excel(self, output_path: Path, exclusions=None, columns=None) -> Tuple[bool, Path]:
        total_records = self.conn.execute("SELECT count(*) FROM (SELECT DISTINCT artikul_norm, brand_norm FROM parts_data) AS t").fetchone()[0]
        if total_records == 0:
            st.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞")
            return False, None

        st.info(f"üì§ –≠–∫—Å–ø–æ—Ä—Ç {total_records:,} –∑–∞–ø–∏—Å–µ–π –≤ Excel...")
        try:
            num_files = (total_records + EXCEL_ROW_LIMIT - 1) // EXCEL_ROW_LIMIT
            base_query = self.get_export_query(exclusions, columns)
            exported_files = []

            progress_bar = st.progress(0, text=f"–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ —ç–∫—Å–ø–æ—Ä—Ç—É {num_files} —Ñ–∞–π–ª–∞(–æ–≤)...")

            for i in range(num_files):
                progress_bar.progress((i + 1) / num_files, text=f"–≠–∫—Å–ø–æ—Ä—Ç —á–∞—Å—Ç–∏ {i+1} –∏–∑ {num_files}...")
                offset = i * EXCEL_ROW_LIMIT
                query = f"{base_query} LIMIT {EXCEL_ROW_LIMIT} OFFSET {offset}"
                df = self.conn.execute(query).pl()

                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —á–∏—Å–ª–æ–≤—ã–µ —Å—Ç–æ–ª–±—Ü—ã –≤ —Å—Ç—Ä–æ–∫–∏
                dimension_cols = ["–î–ª–∏–Ω–Ω–∞", "–®–∏—Ä–∏–Ω–∞", "–í—ã—Å–æ—Ç–∞", "–í–µ—Å", "–î–ª–∏–Ω–Ω–∞/–®–∏—Ä–∏–Ω–∞/–í—ã—Å–æ—Ç–∞", "–ö—Ä–∞—Ç–Ω–æ—Å—Ç—å"]
                for col_name in dimension_cols:
                    if col_name in df.columns:
                        df = df.with_columns(
                            pl.when(pl.col(col_name).is_not_null())
                            .then(pl.col(col_name).cast(pl.Utf8))
                            .otherwise(pl.lit(""))
                            .alias(col_name)
                        )

                file_part_path = output_path.with_name(f"{output_path.stem}_part_{i+1}.xlsx")
                df.write_excel(str(file_part_path))
                exported_files.append(file_part_path)
            progress_bar.empty()

            # –£–ø–∞–∫–æ–≤–∫–∞ –≤ ZIP, –µ—Å–ª–∏ –±–æ–ª—å—à–µ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
            if num_files > 1:
                st.info("–ê—Ä—Ö–∏–≤–∞—Ü–∏—è —Ñ–∞–π–ª–æ–≤ –≤ ZIP...")
                zip_path = output_path.with_suffix('.zip')
                with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                    for file in exported_files:
                        zipf.write(file, arcname=file.name)
                        os.remove(file)
                final_path = zip_path
            else:
                final_path = exported_files[0]
                # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞—Ç—å, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                if final_path != output_path:
                    os.rename(final_path, output_path)
                final_path = output_path

            file_size = os.path.getsize(final_path) / (1024 * 1024)
            st.success(f"‚úÖ –î–∞–Ω–Ω—ã–µ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã: {final_path.name} ({file_size:.1f} –ú–ë)")
            return True, final_path

        except Exception as e:
            logger.exception("–û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ Excel")
            st.error(f"‚ùå –û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ Excel: {e}")
            return False, None
            
    def export_to_parquet(self, output_path: str, exclusions=None, columns=None) -> bool:
        total_records = self.conn.execute("SELECT count(*) FROM (SELECT DISTINCT artikul_norm, brand_norm FROM parts_data) AS t").fetchone()[0]
        if total_records == 0:
            st.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞")
            return False
        st.info(f"üì§ –≠–∫—Å–ø–æ—Ä—Ç {total_records:,} –∑–∞–ø–∏—Å–µ–π –≤ Parquet...")
        try:
            query = self.get_export_query(exclusions, columns)
            df = self.conn.execute(query).pl()
            df.write_parquet(output_path)
            file_size = os.path.getsize(output_path) / (1024 * 1024)
            st.success(f"‚úÖ –î–∞–Ω–Ω—ã–µ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤ Parquet: {output_path} ({file_size:.1f} –ú–ë)")
            return True
        except Exception as e:
            logger.exception("–û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ Parquet")
            st.error(f"‚ùå –û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ Parquet: {e}")
            return False

    def show_export_interface(self):
        st.header("üì§ –£–º–Ω—ã–π —ç–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö")
        total_records = self.conn.execute("SELECT count(DISTINCT (artikul_norm, brand_norm)) FROM parts_data").fetchone()[0]
        st.info(f"–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ (—Å—Ç—Ä–æ–∫): {total_records:,}")
        if total_records == 0:
            st.warning("–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –ø—É—Å—Ç–∞ –∏–ª–∏ –Ω–µ—Ç —Å–≤—è–∑–µ–π –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞. –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ.")
            return
        exclusions = st.text_area("–ò—Å–∫–ª—é—á–∏—Ç—å —Å—Ç—Ä–æ–∫–∏ –ø–æ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏—é (—Ä–∞–∑–¥–µ–ª—è–π—Ç–µ —Å–∏–º–≤–æ–ª–∞–º–∏ '|'):", placeholder="–ü—Ä–∏–º–µ—Ä: —Ñ–∏–ª—å—Ç—Ä –º–∞—Å–ª—è–Ω—ã–π|–ª–∞–º–ø—ã –Ω–∞–∫–∞–ª–∏–≤–∞–Ω–∏—è")
        available_columns = [
            "–ê—Ä—Ç–∏–∫—É–ª –±—Ä–µ–Ω–¥–∞", "–ë—Ä–µ–Ω–¥", "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ", "–ü—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å", "–û–ø–∏—Å–∞–Ω–∏–µ",
            "–ö–∞—Ç–µ–≥–æ—Ä–∏—è —Ç–æ–≤–∞—Ä–∞", "–ö—Ä–∞—Ç–Ω–æ—Å—Ç—å", "–î–ª–∏–Ω–Ω–∞", "–®–∏—Ä–∏–Ω–∞", "–í—ã—Å–æ—Ç–∞",
            "–í–µ—Å", "–î–ª–∏–Ω–Ω–∞/–®–∏—Ä–∏–Ω–∞/–í—ã—Å–æ—Ç–∞", "OE –Ω–æ–º–µ—Ä", "–∞–Ω–∞–ª–æ–≥–∏", "–°—Å—ã–ª–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ"
        ]
        selected_columns = st.multiselect("–í—ã–±–µ—Ä–∏—Ç–µ —Å—Ç–æ–ª–±—Ü—ã –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ (–ø—É—Å—Ç–æ = –≤—Å–µ)", options=available_columns, default=available_columns)
        export_format = st.radio("–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–æ—Ä–º–∞—Ç —ç–∫—Å–ø–æ—Ä—Ç–∞:", ["CSV", "Excel (.xlsx)", "Parquet (–¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤)"], index=0)

        if export_format == "CSV":
            if st.button("üöÄ –≠–∫—Å–ø–æ—Ä—Ç –≤ CSV"):
                output_path = self.data_dir / "auto_parts_report.csv"
                with st.spinner("–ò–¥–µ—Ç —ç–∫—Å–ø–æ—Ä—Ç –≤ CSV..."):
                    success = self.export_to_csv_optimized(str(output_path), exclusions, selected_columns if selected_columns else None)
                if success:
                    with open(output_path, "rb") as f:
                        st.download_button("üì• –°–∫–∞—á–∞—Ç—å CSV —Ñ–∞–π–ª", f, "auto_parts_report.csv", "text/csv")

        elif export_format == "Excel (.xlsx)":
            st.info("‚ÑπÔ∏è –ï—Å–ª–∏ –∑–∞–ø–∏—Å–µ–π –±–æ–ª—å—à–µ 1 –º–ª–Ω, —Ä–µ–∑—É–ª—å—Ç–∞—Ç –±—É–¥–µ—Ç —Ä–∞–∑–¥–µ–ª–µ–Ω –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–∞–π–ª–æ–≤ –∏ —É–ø–∞–∫–æ–≤–∞–Ω –≤ ZIP-–∞—Ä—Ö–∏–≤.")
            if st.button("üìä –≠–∫—Å–ø–æ—Ä—Ç –≤ Excel"):
                output_path = self.data_dir / "auto_parts_report.xlsx"
                with st.spinner("–ò–¥–µ—Ç —ç–∫—Å–ø–æ—Ä—Ç –≤ Excel..."):
                    success, final_path = self.export_to_excel(output_path, exclusions, selected_columns if selected_columns else None)
                if success and final_path and final_path.exists():
                    with open(final_path, "rb") as f:
                        mime = "application/zip" if final_path.suffix == ".zip" else "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                        st.download_button(f"üì• –°–∫–∞—á–∞—Ç—å {final_path.name}", f, final_path.name, mime)

        elif export_format == "Parquet (–¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤)":
            if st.button("‚ö°Ô∏è –≠–∫—Å–ø–æ—Ä—Ç –≤ Parquet"):
                output_path = self.data_dir / "auto_parts_report.parquet"
                with st.spinner("–ò–¥–µ—Ç —ç–∫—Å–ø–æ—Ä—Ç –≤ Parquet..."):
                    success = self.export_to_parquet(str(output_path), exclusions, selected_columns if selected_columns else None)
                if success:
                    with open(output_path, "rb") as f:
                        st.download_button("üì• –°–∫–∞—á–∞—Ç—å Parquet —Ñ–∞–π–ª", f, "auto_parts_report.parquet", "application/octet-stream")

    def delete_by_brand(self, brand_norm: str) -> int:
        try:
            count_result = self.conn.execute("SELECT COUNT(*) FROM parts_data WHERE brand_norm = ?", [brand_norm]).fetchone()
            deleted_count = count_result[0] if count_result else 0
            if deleted_count == 0:
                logger.info(f"No records found for brand: {brand_norm}")
                return 0
            self.conn.execute("DELETE FROM parts_data WHERE brand_norm = ?", [brand_norm])
            self.conn.execute("DELETE FROM cross_references WHERE (artikul_norm, brand_norm) NOT IN (SELECT DISTINCT artikul_norm, brand_norm FROM parts_data)")
            logger.info(f"Deleted {deleted_count} records for brand: {brand_norm}")
            return deleted_count
        except Exception as e:
            logger.error(f"Error deleting by brand {brand_norm}: {e}")
            raise

    def delete_by_artikul(self, artikul_norm: str) -> int:
        try:
            count_result = self.conn.execute("SELECT COUNT(*) FROM parts_data WHERE artikul_norm = ?", [artikul_norm]).fetchone()
            deleted_count = count_result[0] if count_result else 0
            if deleted_count == 0:
                logger.info(f"No records found for artikul: {artikul_norm}")
                return 0
            self.conn.execute("DELETE FROM parts_data WHERE artikul_norm = ?", [artikul_norm])
            self.conn.execute("DELETE FROM cross_references WHERE (artikul_norm, brand_norm) NOT IN (SELECT DISTINCT artikul_norm, brand_norm FROM parts_data)")
            logger.info(f"Deleted {deleted_count} records for artikul: {artikul_norm}")
            return deleted_count
        except Exception as e:
            logger.error(f"Error deleting by artikul {artikul_norm}: {e}")
            raise

    def get_statistics(self) -> Dict:
        stats = {}
        try:
            stats['total_parts'] = self.get_total_records()
            if stats['total_parts'] == 0:
                return {
                    'total_parts': 0, 'total_oe': 0, 'total_brands': 0,
                    'top_brands': pl.DataFrame(), 'categories': pl.DataFrame()
                }
            total_oe_res = self.conn.execute("SELECT COUNT(*) FROM oe_data").fetchone()
            stats['total_oe'] = total_oe_res[0] if total_oe_res else 0
            total_brands_res = self.conn.execute("SELECT COUNT(DISTINCT brand) FROM parts_data WHERE brand IS NOT NULL").fetchone()
            stats['total_brands'] = total_brands_res[0] if total_brands_res else 0
            brand_stats = self.conn.execute("SELECT brand, COUNT(*) as count FROM parts_data WHERE brand IS NOT NULL GROUP BY brand ORDER BY count DESC LIMIT 10").pl()
            stats['top_brands'] = brand_stats
            category_stats = self.conn.execute("SELECT category, COUNT(*) as count FROM oe_data WHERE category IS NOT NULL GROUP BY category ORDER BY count DESC").pl()
            stats['categories'] = category_stats
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–±–æ—Ä–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
            return {
                'total_parts': 0, 'total_oe': 0, 'total_brands': 0,
                'top_brands': pl.DataFrame(), 'categories': pl.DataFrame()
            }
        return stats

    def apply_markup(self, brand: str, percentage: float) -> bool:
        try:
            exists_result = self.conn.execute("SELECT COUNT(*) FROM markup_settings WHERE brand = ?", [brand]).fetchone()
            if exists_result[0] > 0:
                self.conn.execute("UPDATE markup_settings SET markup_percentage = ? WHERE brand = ?", [percentage, brand])
            else:
                self.conn.execute("INSERT INTO markup_settings (brand, markup_percentage) VALUES (?, ?)", [brand, percentage])
            prices_result = self.conn.execute("SELECT artikul_norm, brand_norm, recommended_price FROM prices WHERE brand_norm = ?", [brand]).fetchall()
            updated_prices = []
            for row in prices_result:
                artikul_norm, brand_norm, old_price = row
                new_price = round(old_price * (1 + percentage / 100), 2)
                updated_prices.append((new_price, artikul_norm, brand_norm))
            if updated_prices:
                self.conn.executemany("UPDATE prices SET recommended_price = ? WHERE artikul_norm = ? AND brand_norm = ?", updated_prices)
            return True
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ –Ω–∞—Ü–µ–Ω–∫–∏: {e}")
            return False

    def show_management_interface(self):
        st.header("üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö")
        st.subheader("–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—â–µ–π –Ω–∞—Ü–µ–Ω–∫–∏")
        global_markup = st.number_input("–û–±—â–∞—è –Ω–∞—Ü–µ–Ω–∫–∞ (%)", min_value=0.0, max_value=100.0, value=0.0, step=0.1)
        if st.button("–ü—Ä–∏–º–µ–Ω–∏—Ç—å –æ–±—â—É—é –Ω–∞—Ü–µ–Ω–∫—É –∫–æ –≤—Å–µ–º –±—Ä–µ–Ω–¥–∞–º"):
            self.conn.execute("DELETE FROM markup_settings")
            affected_rows = self.conn.execute("UPDATE prices SET recommended_price = recommended_price * (1 + ? / 100)", [global_markup]).rowcount
            st.success(f"–ü—Ä–∏–º–µ–Ω–µ–Ω–∞ –æ–±—â–∞—è –Ω–∞—Ü–µ–Ω–∫–∞ {global_markup}% –∫ {affected_rows} –∑–∞–ø–∏—Å—è–º.")
        st.subheader("–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–æ–π –Ω–∞—Ü–µ–Ω–∫–∏ –ø–æ –±—Ä–µ–Ω–¥–∞–º")
        brand = st.text_input("–ù–∞–∑–≤–∞–Ω–∏–µ –±—Ä–µ–Ω–¥–∞ (–±–µ–∑ –Ω–æ—Ä–º–∏—Ä–æ–≤–∫–∏):")
        if brand:
            norm_brand = self.normalize_key(pl.Series([brand]))[0]
            current_markup = self.conn.execute("SELECT markup_percentage FROM markup_settings WHERE brand = ?", [norm_brand]).fetchone()
            current_markup_value = current_markup[0] if current_markup else 0.0
            brand_markup = st.number_input(f"–ù–∞—Ü–µ–Ω–∫–∞ –¥–ª—è –±—Ä–µ–Ω–¥–∞ '{brand}' (%)", min_value=0.0, max_value=100.0, value=float(current_markup_value), step=0.1)
            if st.button("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—É—é –Ω–∞—Ü–µ–Ω–∫—É"):
                success = self.apply_markup(norm_brand, brand_markup)
                if success:
                    st.success(f"–£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–∞—è –Ω–∞—Ü–µ–Ω–∫–∞ {brand_markup}% –¥–ª—è –±—Ä–µ–Ω–¥–∞ '{brand}'.")
                else:
                    st.error("–û—à–∏–±–∫–∞ –ø—Ä–∏ —É—Å—Ç–∞–Ω–æ–≤–∫–µ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–æ–π –Ω–∞—Ü–µ–Ω–∫–∏.")


def main():
    st.set_page_config(page_title="AutoParts Catalog 10M+", layout="wide", page_icon="üöó")
    st.title("üöó AutoParts Catalog - –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è 10+ –º–ª–Ω –∑–∞–ø–∏—Å–µ–π")
    st.markdown("""
    ### üí™ –ú–æ—â–Ω–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –±–æ–ª—å—à–∏–º–∏ –æ–±—ä–µ–º–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö –∞–≤—Ç–æ–∑–∞–ø—á–∞—Å—Ç–µ–π
    - **–ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è**: –ë–µ–∑–æ–ø–∞—Å–Ω–æ –¥–æ–±–∞–≤–ª—è–π—Ç–µ –Ω–æ–≤—ã–µ —Ñ–∞–π–ª—ã –¥–ª—è –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∫–∞—Ç–∞–ª–æ–≥–∞.
    - **–ù–∞–¥–µ–∂–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ**: –î–∞–Ω–Ω—ã–µ –∏–∑ 5-—Ç–∏ —Ç–∏–ø–æ–≤ —Ñ–∞–π–ª–æ–≤ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Å–ª–∏–≤–∞—é—Ç—Å—è –≤ –µ–¥–∏–Ω—É—é –±–∞–∑—É.
    - **–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ö—Ä–∞–Ω–µ–Ω–∏–µ**: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ DuckDB –¥–ª—è –º–≥–Ω–æ–≤–µ–Ω–Ω–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞ –∏ –∞–Ω–∞–ª–∏–∑–∞.
    - **–£–º–Ω—ã–π —ç–∫—Å–ø–æ—Ä—Ç**: –ë—ã—Å—Ç—Ä—ã–π –∏ –Ω–∞–¥–µ–∂–Ω—ã–π —ç–∫—Å–ø–æ—Ä—Ç –≤ CSV, Excel –∏–ª–∏ Parquet —Å –≥–∞—Ä–∞–Ω—Ç–∏–µ–π –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤.
    """)
    
    catalog = HighVolumeAutoPartsCatalog()
    st.sidebar.title("üß≠ –ù–∞–≤–∏–≥–∞—Ü–∏—è")
    menu_option = st.sidebar.radio("–í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ:", ["–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö", "–≠–∫—Å–ø–æ—Ä—Ç", "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞", "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã–º–∏"])
    
    if menu_option == "–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö":
        st.header("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
        st.info("""
        **–ü–æ—Ä—è–¥–æ–∫ —Ä–∞–±–æ—Ç—ã:**
        1. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –æ–¥–∏–Ω –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–∞–π–ª–æ–≤ Excel (`.xlsx`, `.xls`). –ù–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –∑–∞–≥—Ä—É–∂–∞—Ç—å –≤—Å–µ —Å—Ä–∞–∑—É.
        2. –ù–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É "–ù–∞—á–∞—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É".
        3. –°–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–æ—á–∏—Ç–∞–µ—Ç, –æ–±—ä–µ–¥–∏–Ω–∏—Ç –¥–∞–Ω–Ω—ã–µ –∏ –æ–±–Ω–æ–≤–∏—Ç/–¥–æ–ø–æ–ª–Ω–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –±–∞–∑—É.
        ...
        """)
        col1, col2 = st.columns(2)
        with col1:
            oe_file = st.file_uploader("1. –û—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (OE)", type=['xlsx', 'xls'])
            cross_file = st.file_uploader("2. –ö—Ä–æ—Å—Å—ã (OE -> –ê—Ä—Ç–∏–∫—É–ª)", type=['xlsx', 'xls'])
            barcode_file = st.file_uploader("3. –®—Ç—Ä–∏—Ö-–∫–æ–¥—ã –∏ –∫—Ä–∞—Ç–Ω–æ—Å—Ç—å", type=['xlsx', 'xls'])
        with col2:
            dimensions_file = st.file_uploader("4. –í–µ—Å–æ–≥–∞–±–∞—Ä–∏—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ", type=['xlsx', 'xls'])
            images_file = st.file_uploader("5. –°—Å—ã–ª–∫–∏ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è", type=['xlsx', 'xls'])
            prices_file = st.file_uploader("6. –¶–µ–Ω—ã (—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—ã–µ)", type=['xlsx', 'xls'])
        file_map = {
            'oe': oe_file, 'cross': cross_file, 'barcode': barcode_file,
            'dimensions': dimensions_file, 'images': images_file, 'prices': prices_file
        }
        if st.button("üöÄ –ù–∞—á–∞—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–∞–Ω–Ω—ã—Ö"):
            paths_to_process = {}
            any_file_uploaded = False
            for ftype, uploaded_file in file_map.items():
                if uploaded_file:
                    any_file_uploaded = True
                    path = catalog.data_dir / f"{ftype}_data_{int(time.time())}_{uploaded_file.name}"
                    with open(path, "wb") as f:
                        f.write(uploaded_file.getvalue())
                    paths_to_process[ftype] = str(path)
            if any_file_uploaded:
                stats = catalog.merge_all_data_parallel(paths_to_process)
                if stats:
                    st.subheader("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏")
                    st.metric("–û–±—â–µ–µ –≤—Ä–µ–º—è", f"{stats.get('processing_time', 0):.2f} —Å–µ–∫")
                    st.metric("–í—Å–µ–≥–æ –∞—Ä—Ç–∏–∫—É–ª–æ–≤ –≤ –±–∞–∑–µ", f"{stats.get('total_records', 0):,}")
                    st.metric("–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤", f"{len(paths_to_process)}")
            else:
                st.warning("‚ö†Ô∏è –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω —Ñ–∞–π–ª –¥–ª—è –Ω–∞—á–∞–ª–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏.")
    elif menu_option == "–≠–∫—Å–ø–æ—Ä—Ç":
        catalog.show_export_interface()
    elif menu_option == "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞":
        st.header("üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–∞–ª–æ–≥—É")
        with st.spinner("–°–±–æ—Ä —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏..."):
            stats = catalog.get_statistics()
        if stats.get('total_parts', 0) > 0:
            st.metric("–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∞—Ä—Ç–∏–∫—É–ª–æ–≤", f"{stats.get('total_parts', 0):,}")
            st.metric("–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö OE", f"{stats.get('total_oe', 0):,}")
            st.metric("–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –±—Ä–µ–Ω–¥–æ–≤", f"{stats.get('total_brands', 0):,}")
            st.subheader("üèÜ –¢–æ–ø-10 –±—Ä–µ–Ω–¥–æ–≤ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –∞—Ä—Ç–∏–∫—É–ª–æ–≤")
            if 'top_brands' in stats and not stats['top_brands'].is_empty():
                st.dataframe(stats['top_brands'].to_pandas(), width='stretch')
            else:
                st.write("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ –±—Ä–µ–Ω–¥–∞–º.")
            st.subheader("üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º")
            if 'categories' in stats and not stats['categories'].is_empty():
                st.bar_chart(stats['categories'].to_pandas().set_index('category'))
            else:
                st.write("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º.")
        else:
            st.info("–î–∞–Ω–Ω—ã–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç. –ü–µ—Ä–µ–π–¥–∏—Ç–µ –≤ —Ä–∞–∑–¥–µ–ª '–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö', —á—Ç–æ–±—ã –Ω–∞—á–∞—Ç—å.")
    elif menu_option == "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã–º–∏":
        catalog.show_management_interface()

if __name__ == "__main__":
    main()
